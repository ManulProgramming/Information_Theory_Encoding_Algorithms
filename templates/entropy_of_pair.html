<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Theory</title>
    <style>
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        body {
            font-family: Arial, sans-serif;
            background-color: #f2f2f2;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            overflow: hidden;
        }
        .container {
            display: flex;
            flex-direction: row;
            width: 90%;
            max-width: 1920px;
            height: 90vh;
            background-color: #e0e0e0;
            padding: 20px;
            border-radius: 15px;
            overflow: hidden;
        }
        .main {
            flex: 3;
            padding: 20px;
            background-color: white;
            border-radius: 15px;
            margin-right: 20px;
            overflow-y: auto;
        }
        img {
            max-width: 90%;
        }
        .tabs {
            flex: 1;
            padding: 20px;
            background-color: white;
            border-radius: 15px;
            overflow-y: auto;
        }
        h2 {
            margin-bottom: 15px;
        }
        .input-box {
            width: 100%;
            padding: 10px;
            margin-bottom: 20px;
            border: 1px solid #bfbfbf;
            border-radius: 5px;
        }
        .result {
            margin-bottom: 15px;
        }
        .divider {
            height: 4px;
            background-color: orange;
            width: 100%;
            margin-bottom: 20px;
        }
        .additional-content {
            margin-bottom: 20px;
        }
        .tabs-list {
            list-style-type: none;
        }
        .tabs-list li {
            margin-bottom: 10px;
        }
	    a {
	        color: black;
	        text-decoration: none;
	    }

        @media (max-width: 768px) {
            body {
                height: auto;
                overflow: auto;
            }
            .container {
                flex-direction: column;
                align-items: center;
		        margin-top: 20px;
                width: 95%;
                height: auto;
            }
            .main, .tabs {
                width: 95%;
                margin-right: 0;
                margin-bottom: 20px;
                max-height: 50vh;
            }
            .main, .tabs {
                overflow-y: auto;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="main">
            <h2>Entropy of pair</h2>
            <form id="input-form" method="POST" action="/submit">
                <input class="input-box" type="text" name="input-box" id="input-box" placeholder="Input a sentence...">
                <input type="hidden" type="text" name="typeof" value="Entropy_of_pair"/>
            </form>
            <div class="result">
                <strong>Result:</strong></br></br>
                <p style="color: red" id="error"></p>
                <strong>&emsp;Plain text: </strong>{{ result_entropy_of_pair0 }}<br><br>
                &emsp;{{ result_entropy_of_pair1 }}
            </div>
            <div class="divider"></div>
            <div class="additional-content">
                <h3>Information</h3>
		        <br>
                <p>&emsp;Calculating text entropy taking into account correlations, i.e. the entropy of pairs of symbols or even longer sequences, allows for a more accurate and in-depth assessment of the uncertainty and information structure of the text. Here are some key aspects of why this is important:</p>
                <br>
                <p>&emsp;1. <b>Taking into account dependencies between symbols:</b></p>
                <p>&emsp;&emsp;- Languages have statistical dependencies between symbols. For example, in English, the letter "q" is almost always followed by "u". Entropy calculated without taking these correlations into account (only for individual symbols) can overestimate uncertainty. When pairs of symbols are taken into account, entropy becomes more accurate, reflecting the real amount of information.</p>
                <p>&emsp;2. <b>A more accurate assessment of coding efficiency:</b></p>
                <p>&emsp;&emsp;- Understanding the structure of dependencies allows us to develop more efficient encoding algorithms. Encoding based on models that take into account pairs of symbols or even longer sequences can significantly reduce the number of bits needed to represent text compared to encoding each symbol independently.</p>
                <p>&emsp;3. <b>Improved compression algorithms:</b></p>
                <p>&emsp;&emsp;- Compression algorithms that use statistical information about character sequences (such as algorithms based on a context model) can achieve better compression ratios. Knowing which characters or pairs of characters are more likely to appear together allows data to be compressed more efficiently.</p>
                <p>&emsp;4. <b>Better understanding of the language:</b></p>
                <p>&emsp;&emsp;- Analyzing correlations between symbols can reveal linguistic features and rules, which is useful not only in information theory but also in linguistics, machine learning, text analysis, and other fields.</p>
                <p>&emsp;5. <b>Application in machine learning and artificial intelligence:</b></p>
                <p>&emsp;&emsp;- In machine learning models such as neural networks that process text for translation, summarization , or text generation tasks, taking into account correlations between characters helps create more accurate and natural language models.</p>
                <br>
                <p>&emsp;Thus, the calculation of entropy, taking into account correlation links, not only increases the accuracy of the assessment of the information content of the text, but also contributes to the development of more advanced algorithms for processing and transmitting information.</p>
		        <br>
                <p>&emsp;To calculate the entropy of character pairs, a formula similar to Shannon&#39;s basic formula is used, but applied to pairs of characters. This helps to account for the mutual dependencies between characters in the text. Here are the steps and formula for calculating the entropy of character pairs:</p>
                <p>&emsp;1. <b>Determining the probabilities of pairs of symbols:</b></p>
                <p>&emsp;- For each possible pair of symbols <i>(xi, xj)</i> in the text, the probability <i>p(xi, xj)</i> that symbol <i>xi</i> follows symbol <i>x j is determined</i>.</p>
                <p>&emsp;2. <b>Entropy formula for pairs of symbols:</b></p>
                <p>&emsp;- The entropy of pairs of symbols <i>H(X, Y)</i> is calculated using the formula:</p>
                <br>
                &emsp;<img src="/static/entropy_of_pair_formula.png">
		        <br>
                <p>&emsp;where <i>X</i> and <i>Y</i> denote sets of symbols, and the summation is performed over all pairs of symbols <i>(xi, xj)</i> that occur in the text.</p>
		        <br><br>
		        <h3>Example:</h3>
		        <br>
		        <p>&emsp;Input: <i>LOREM IPSUM</i></p>
		        <br>
		        <p>&emsp;Result: <i>3.321928094887362</i></p>
            </div>
        </div>
        <div class="tabs">
            <h2>Tabs</h2>
            <ul class="tabs-list">
                <a href="/"><li>Home</li></a>
                <a href="/entropy"><li>&emsp;1. Entropy</li></a>
                <a href="/entropy_of_pair"><li>&emsp;2. Entropy of pair</li></a>
                <a href="/rle"><li>&emsp;3. RLE</li></a>
                <a href="/lzw"><li>&emsp;4. LZW</li></a>
                <a href="/huffman"><li>&emsp;5. Huffman algorithm</li></a>
                <a href="/arithmetic_coding"><li>&emsp;6. Arithmetic coding</li></a>
                <a href="/shannon_fano"><li>&emsp;7. Shannon-Fano algorithm</li></a>
                <a href="/color_histogram"><li>Project</li></a>
            </ul>
        </div>
    </div>
    <script>
        document.getElementById('input-form').addEventListener('submit', function(event) {
            const input = document.getElementById('input-box').value;
            if (input.length > 512){
                event.preventDefault();
                document.getElementById('error').innerText = 'Input string exceeds 512 character limit.';
            }
        });
        function checkSubmit(e) {
            if (e.keyCode === 13) {
                document.getElementById('input-form').submit();
            }
        }
    </script>
</body>
</html>